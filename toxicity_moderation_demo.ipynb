{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b9d359b3",
   "metadata": {},
   "source": [
    "# ðŸ›¡ï¸ NLP Toxicity Detection - Live Test\n",
    "\n",
    "This notebook implements the core logic of the **Low-Latency Moderation API**. It loads the configured toxicity classification model and provides an interactive interface to test messages against the moderation rules.\n",
    "\n",
    "> **Note:** This notebook acts as a simplified demonstration of the mobile app's moderation system. It allows for viewing and testing the model logic independently and serves as a fallback or debugging tool in case of issues with the main application.\n",
    "\n",
    "### **How it works**\n",
    "I have split the implementation into logical blocks to explain each step:\n",
    "1.  **Install Libraries**: Setting up the environment.\n",
    "2.  **Imports**: Loading necessary modules.\n",
    "3.  **Configuration**: defining model parameters.\n",
    "4.  **Services**: Loading the AI model.\n",
    "5.  **Logic**: Business rules for blocking/allowing messages.\n",
    "6.  **Interactive UI**: A simple testing widget."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63f7759e",
   "metadata": {},
   "source": [
    "### 1. Install Necessary Libraries\n",
    "I start by installing the external libraries required for this project/demo.\n",
    "*   `transformers`: Provides the pre-trained NLP models (Hugging Face) for text classification.\n",
    "*   `pydantic`: Used for defining data models ensuring my inputs and outputs are strictly typed and valid.\n",
    "*   `ipywidgets`: Enables interactive UI controls (like text boxes and buttons) directly within this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d82ce147",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Install necessary libraries\n",
    "!pip install -q transformers pydantic ipywidgets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "476a1c4b",
   "metadata": {},
   "source": [
    "### 2. Imports\n",
    "Here I import the standard and third-party modules.\n",
    "*   `os`, `functools`, `typing`: Standard Python utilities for file paths, caching, and type hinting.\n",
    "*   `transformers`: Classes to load the specific model architecture (`AutoModelForSequenceClassification`) and tokenizer (`AutoTokenizer`).\n",
    "*   `pydantic`: `BaseModel` to create my structured data classes.\n",
    "*   `ipywidgets` & `IPython.display`: For building the user interface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c2ca64c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Imports\n",
    "import os\n",
    "from typing import Literal, TypedDict\n",
    "from functools import lru_cache\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, TextClassificationPipeline\n",
    "from pydantic import BaseModel, Field\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2344542",
   "metadata": {},
   "source": [
    "### 3. Configuration & Data Models\n",
    "I define a `Settings` class to centralize configuration (like the specific model name and confidence threshold).\n",
    "\n",
    "I also define Pydantic models:\n",
    "*   `ToxicityResult`: Captures the raw output from the model (label and confidence).\n",
    "*   `ModerationResponse`: Represents the final decision sent back to the application (status accepted/rejected, message, etc.). Structure ensures consistency across the app."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bac30f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Configuration ---\n",
    "class Settings:\n",
    "    model_name: str = \"martin-ha/toxic-comment-model\"\n",
    "    negative_threshold: float = 0.70\n",
    "\n",
    "settings = Settings()\n",
    "\n",
    "# --- Models ---\n",
    "class ToxicityResult(BaseModel):\n",
    "    label: Literal[\"TOXIC\", \"NON-TOXIC\"]\n",
    "    confidence: float\n",
    "\n",
    "class ModerationResponse(BaseModel):\n",
    "    status: Literal[\"accepted\", \"rejected\"]\n",
    "    message: str\n",
    "    label: Literal[\"TOXIC\", \"NON-TOXIC\"]\n",
    "    confidence: float"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "822c3f13",
   "metadata": {},
   "source": [
    "### 4. Model Loading & Services\n",
    "I load the pre-trained model and tokenizer from Hugging Face.\n",
    "*   I use `AutoTokenizer` and `AutoModelForSequenceClassification` which automatically detect the correct architecture for \"martin-ha/toxic-comment-model\".\n",
    "*   I wrap the prediction logic in `classify_message` to handle the raw pipeline call and format it into the `ToxicityResult` model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da38d7ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Services ---\n",
    "print(f\"ðŸ”„ Loading model: {settings.model_name}...\")\n",
    "\n",
    "# Load Model & Tokenizer (Cached globally)\n",
    "try:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(settings.model_name)\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(settings.model_name)\n",
    "    pipeline = TextClassificationPipeline(model=model, tokenizer=tokenizer)\n",
    "    print(\"âœ… Model loaded successfully!\")\n",
    "except Exception as e:\n",
    "    print(f\" âŒ Error loading model: {e}\")\n",
    "    pipeline = None\n",
    "\n",
    "def classify_message(text: str) -> ToxicityResult:\n",
    "    \"\"\"Runs toxicity classification on text.\"\"\"\n",
    "    if not pipeline:\n",
    "        raise RuntimeError(\"Model pipeline not initialized.\")\n",
    "    \n",
    "    # Run prediction\n",
    "    result = pipeline(text)[0]\n",
    "    raw_label = result[\"label\"].upper()\n",
    "    confidence = float(result[\"score\"])\n",
    "    \n",
    "    return ToxicityResult(label=raw_label, confidence=confidence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22ff6ec6",
   "metadata": {},
   "source": [
    "### 5. Main Moderation Logic\n",
    "The `check_message` function contains the business rules.\n",
    "*   It calls `classify_message` to get the raw probability.\n",
    "*   **Blocking Rule**: If the text is toxic AND confidence is high (â‰¥ 0.70), I reject it.\n",
    "*   **Flagging Rule**: If toxic but lower confidence, I might log it but ultimately still pass it (or reject depending on policy). Here, I default to setting status to \"rejected\" for toxic labels but distinguishing slightly in the message or final handling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da62f5d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Main Logic ---\n",
    "def check_message(text: str) -> ModerationResponse:\n",
    "    \"\"\"Classify the incoming text and block highly confident toxic content.\"\"\"\n",
    "    result = classify_message(text)\n",
    "\n",
    "    # BLOCK if TOXIC and High Confidence (>= 0.70)\n",
    "    if result.label == \"TOXIC\" and result.confidence >= settings.negative_threshold:\n",
    "        return ModerationResponse(\n",
    "            status=\"rejected\",\n",
    "            message=\"â›” Message classified as toxic.\",\n",
    "            label=result.label,\n",
    "            confidence=result.confidence,\n",
    "        )\n",
    "\n",
    "    # Note: Even if TOXIC (but low confidence), rejected but message to passed.\n",
    "    status = \"accepted\"\n",
    "    return ModerationResponse(\n",
    "        status=status,\n",
    "        message=\"âœ… Message passed moderation.\",\n",
    "        label=result.label,\n",
    "        confidence=result.confidence,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3677a39e",
   "metadata": {},
   "source": [
    "### 6. Interactive Interface\n",
    "Finally, I use `ipywidgets` to create a simple text area and button. This allows you to test the model immediately without needing a frontend application.\n",
    "*   `on_button_click` handles the interaction, runs the check, and prints a user-friendly result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Interactive UI ---\n",
    "input_box = widgets.Textarea(\n",
    "    value='',\n",
    "    placeholder='Type a message here to test moderation...',\n",
    "    description='Message:',\n",
    "    layout=widgets.Layout(width='60%', height='100px')\n",
    ")\n",
    "\n",
    "check_button = widgets.Button(\n",
    "    description='Check Toxicity',\n",
    "    button_style='primary', # 'success', 'info', 'warning', 'danger'\n",
    "    icon='check'\n",
    ")\n",
    "\n",
    "output_area = widgets.Output()\n",
    "\n",
    "def on_button_click(b):\n",
    "    with output_area:\n",
    "        clear_output()\n",
    "        message = input_box.value.strip()\n",
    "        if not message:\n",
    "            print(\"âš ï¸ Please enter a message first.\")\n",
    "            return\n",
    "            \n",
    "        try:\n",
    "            print(f\"Analyzing: '{message}' ...\\n\")\n",
    "            response = check_message(message)\n",
    "            \n",
    "            # Display Result\n",
    "            print(f\"--- RESULT ---\")\n",
    "            print(f\"Status:     {response.status.upper()}\")\n",
    "            print(f\"Label:      {response.label}\")\n",
    "            print(f\"Confidence: {response.confidence:.4f}\")\n",
    "            print(f\"Response:   {response.message}\")\n",
    "            \n",
    "            # Visual Indicator\n",
    "            if response.status == \"rejected\" and response.confidence >= settings.negative_threshold:\n",
    "                 print(\"\\nðŸ”´ BLOCKED (High Confidence)\")\n",
    "            elif response.label == \"TOXIC\":\n",
    "                 print(\"\\nðŸŸ  FLAGGED (Low Confidence)\")\n",
    "            else:\n",
    "                 print(\"\\nðŸŸ¢ CLEAN\")\n",
    "                 \n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e}\")\n",
    "\n",
    "check_button.on_click(on_button_click)\n",
    "\n",
    "# Display the widget\n",
    "display(widgets.VBox([input_box, check_button, output_area]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
